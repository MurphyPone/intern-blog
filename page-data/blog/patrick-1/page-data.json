{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/patrick-1","webpackCompilationHash":"9320626301566938797c","result":{"data":{"markdownRemark":{"html":"<p>Hi there. If you are reading this, it's probably because you are interested in also becoming a big brain.\nOr you are Braden looking to make yourself look even more big brain. In either case, welcome. Reinforcement Learning\nis a super cool but also super complicated field within machine learning. Sutton and Barto define it as:</p>\n<blockquote>\n<p>\"Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate\nreward but also the next situation and, through that, all subsequent rewards. These two characteristics: trial-and-error search and delayed reward, are the two most important distinguishing features of reinforcement learning.\"</p>\n</blockquote>\n<p>Basically, you've decided to take all the hard parts of machine learning, such as teaching a computer to learn, and made it <em>much, much, much</em> harder by not giving it any examples to begin with. You're doing the mathematical equivalent of the Spartan practice of leaving babies to die on the cliff and expecting it to learn to do complex things like play Atari games or predict MRI scans.</p>\n<p>Luckily, math is on your side. With the power of gradients and expectations, you can actually approach an optimal solution in a reasonable amount of time in a number of ways. There are countless variations on these core algorithms that make them train faster and become more efficient at using examples. But before we jump into that, we need to introduce some terminology.</p>\n<h3>Terms to Know</h3>\n<ul>\n<li>\n<p>Environment: World that the agent lives in, contains all the information that exists to the agent. May change both independently of the agent and based on the action of the agent.</p>\n</li>\n<li>\n<p>Agent: What observes the environment and takes an action based on that observation. Also receives a \"reward\" signal from the environment sometimes when it performs an action</p>\n</li>\n<li>\n<p>State (<strong><em>s</em></strong>): Complete description of the state of the world, sometimes also used to refer what the agent <em>can</em> see in the environment (really the observation). Depending on the environment, the state can be <em>fully observable</em> or <em>partially obserable</em></p>\n</li>\n<li>\n<p>Action/Action Space (<strong><em>a</em></strong>): The range of actions which an agent can take within an environment. The two types of action spaces are <em>discrete</em>, where there are a finite number of actions to choose from, or <em>continuous</em>, where there as an infinite number of possible actions to take. A discrete space would be something like an Atari game, while a continuous space would involve something like adjustable torque.</p>\n</li>\n<li>\n<p>Policy: The function or rules by which an actor chooses and action <em>a</em> based on the state. It is usually represented by <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>π</mi></mrow><annotation encoding=\"application/x-tex\">\\pi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">π</span></span></span></span>(<em>s</em>) for stochastic policies or <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>(<em>s</em>) for deterministic policies.</p>\n<blockquote>\n<p>\"In deterministic models, the output of the model is fully determined by the parameter values and the initial conditions. Stochastic models possess some inherent randomness.\" (from the interwebs)</p>\n</blockquote>\n</li>\n<li>\n<p>Trajectory: A sequence <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"script\">T</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.25417em;\">T</span></span></span></span></span> of some length of the states <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"script\">S</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.075em;\">S</span></span></span></span></span> and actions <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"script\">A</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathcal\">A</span></span></span></span></span> of the environment as it progresses through time or by frame. Otherwise known as episodes or rollouts.</p>\n</li>\n</ul>\n<p>For a more extensive glossary of terms and functions, visit Peter's <a href=\"https://murphypone.github.io/intern-blog/blog/peter-7\">Terrible Turing Machines - 07</a>, where he has an extensive list of terms and crap you need to know to feel big brain.</p>\n<p><strong>Now that you have some knowledge of the basic terminology of RL, we are going to crack open the black box and see how this magical subject works.</strong></p>\n<p>Some may say I'm a dreamer, but I'm not the only one.</p>","frontmatter":{"date":"21 June, 2019","path":"/blog/patrick-1","title":"Reinforcement-Learning: Feeling Big Brain All the Time","author":"Bob Saget, Sergey Levine."},"fields":{"readingTime":{"text":"4 min read"}}}},"pageContext":{"isCreatedByStatefulCreatePages":false}}}